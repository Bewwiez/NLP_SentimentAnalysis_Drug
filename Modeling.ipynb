{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07406620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import numpy as np \n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7b6b0",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57647360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_Path = 'drugLibTrain_raw.tsv'\n",
    "test_file_Path = 'drugLibTest_raw.tsv'\n",
    "drug_train_df = pd.read_csv(train_file_Path, sep = '\\t')\n",
    "drug_test_df = pd.read_csv(test_file_Path,sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b03c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train_df['benefitsReview'] = drug_train_df['benefitsReview'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e454339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_to_sentiment(ratings):\n",
    "    if ratings >= 8:\n",
    "        return 2\n",
    "    elif ratings <= 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fec6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train_df['sentiment_label'] = drug_train_df['rating'].apply(turn_to_sentiment)\n",
    "drug_test_df['sentiment_label'] = drug_test_df['rating'].apply(turn_to_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b5755",
   "metadata": {},
   "source": [
    "## Paths for things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d7c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_Path = 'dmis-lab/biobert-v1.1' \n",
    "finetune_output = \"./sentiment_finetuning_cv\"\n",
    "final_model_output = \"./final_sentiment_model\"\n",
    "label_column = 'sentiment'\n",
    "num_unique_Labels = 3\n",
    "labels = ['negative','neutral','positive']\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "# update the reviews (side effect or benefits)\n",
    "type_review = 'benefitsReview'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4095709",
   "metadata": {},
   "source": [
    "## Functions for modeling and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22e03801",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(model_Path)\n",
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_Path,\n",
    "        num_labels = num_unique_Labels,\n",
    "        id2label = id2label,\n",
    "        label2id =label2id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba4e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    list_form = review.tolist()\n",
    "    return tokenizer(\n",
    "    list_form,\n",
    "    max_length=512, \n",
    "    truncation=True,             \n",
    "    padding=\"max_length\",        \n",
    "    return_tensors=\"pt\"         \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2578e4e9",
   "metadata": {},
   "source": [
    "### Training Arguments Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f328bcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2e-5\n",
    "epoch_Numb = 10\n",
    "batch_size = 8\n",
    "training_Args = TrainingArguments(\n",
    "output_dir=finetune_output,\n",
    "num_train_epochs=epoch_Numb,\n",
    "learning_rate=LR,\n",
    "per_device_train_batch_size=batch_size,\n",
    "per_device_eval_batch_size=batch_size,\n",
    "warmup_steps=100,\n",
    "weight_decay=0.01,\n",
    "logging_steps=50,\n",
    "eval_strategy=\"epoch\",\n",
    "save_strategy=\"epoch\",\n",
    "load_best_model_at_end=True,\n",
    "metric_for_best_model=\"f1_weighted\",\n",
    "save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3aa4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred: EvalPrediction):\n",
    "    predictions, label_ids = eval_pred\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)\n",
    "    accuracy = accuracy_score(y_true=label_ids, y_pred=predicted_ids)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true=label_ids, y_pred=predicted_ids, average='weighted', zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1,\n",
    "        'precision_weighted': precision,\n",
    "        'recall_weighted': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71bd3e",
   "metadata": {},
   "source": [
    "##### just testing the above function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b196c3",
   "metadata": {},
   "source": [
    "##### HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cb267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_Benefits = tokenize(drug_train_df['benefitsReview'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9f088",
   "metadata": {},
   "source": [
    "### HF Training Benefits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b28d07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train_df_sentiment_Label = drug_train_df['sentiment_label']\n",
    "review_train_Dict = {\"input_ids\" : tokenized_Benefits['input_ids'], \"token_type_ids\": tokenized_Benefits['token_type_ids'],\"attention_mask\": tokenized_Benefits['attention_mask'], \"labels\": torch.tensor(drug_train_df_sentiment_Label)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e8a01",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9145c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5 \n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "all_fold_metrics = []\n",
    "y_split_labels = drug_train_df_sentiment_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b43c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_train_benefits_df = drug_train_df[['benefitsReview','sentiment_label']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Starting Fold 1/5 =====\n",
      "Tokenizing training data for fold 1...\n",
      "Tokenizing validation data for fold 1...\n",
      "Fold 1: Train dataset size: 2485, Eval dataset size: 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/cs/9f887k5s41b2tl4z2mhbpy400000gp/T/ipykernel_4764/1498861140.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_fold = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Long/Documents/NLP_SentimentAnalysis_Drug/nlp_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='3110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/3110 : < :, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for fold_num, (train_idx, val_idx) in enumerate(skf.split(drug_train_benefits_df['benefitsReview'], y_split_labels)):\n",
    "    print(f\"\\n===== Starting Fold {fold_num + 1}/{N_SPLITS} =====\")\n",
    "    df_train_fold = drug_train_benefits_df.iloc[train_idx]\n",
    "    df_val_fold = drug_train_benefits_df.iloc[val_idx]\n",
    "    #tokenize the reviews\n",
    "    print(f\"Tokenizing training data for fold {fold_num + 1}...\")\n",
    "    tokenized_train_inputs = tokenize(df_train_fold['benefitsReview'])\n",
    "    print(f\"Tokenizing validation data for fold {fold_num + 1}...\")\n",
    "    tokenized_val_inputs = tokenize(df_val_fold['benefitsReview'])\n",
    "\n",
    "    #labels\n",
    "    train_labels_fold = torch.tensor(df_train_fold['sentiment_label'].tolist())\n",
    "    val_labels_fold = torch.tensor(df_val_fold['sentiment_label'].tolist())\n",
    "\n",
    "    train_data_dict = {\n",
    "        'input_ids': tokenized_train_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_train_inputs['attention_mask'],\n",
    "        'token_type_ids': tokenized_train_inputs['token_type_ids'],\n",
    "        'labels': train_labels_fold\n",
    "    }\n",
    "\n",
    "    train_dataset_fold = Dataset.from_dict(train_data_dict)\n",
    "\n",
    "    val_data_dict = {\n",
    "        'input_ids': tokenized_val_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_val_inputs['attention_mask'],\n",
    "        'token_type_ids': tokenized_val_inputs['token_type_ids'],\n",
    "        'labels': val_labels_fold\n",
    "    }\n",
    "    \n",
    "    eval_dataset_fold = Dataset.from_dict(val_data_dict)\n",
    "\n",
    "    print(f\"Fold {fold_num + 1}: Train dataset size: {len(train_dataset_fold)}, Eval dataset size: {len(eval_dataset_fold)}\")\n",
    "\n",
    "    model_fold = get_model()\n",
    "\n",
    "    fold_output_dir = f\"{finetune_output}/fold_{fold_num + 1}\"\n",
    "\n",
    "    trainer_fold = Trainer(\n",
    "        model=model_fold,\n",
    "        args=training_Args, # Use the fold-specific args\n",
    "        train_dataset=train_dataset_fold,\n",
    "        eval_dataset=eval_dataset_fold,\n",
    "        compute_metrics=compute_classification_metrics,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    print(f\"Training fold {fold_num + 1}...\") # Add this to see if it's reached\n",
    "    trainer_fold.train() # <<<< MAKE SURE THIS LINE IS PRESENT AND NOT COMMENTED OUT\n",
    "\n",
    "    print(f\"Evaluating fold {fold_num + 1}...\") # Add this to see if it's reached\n",
    "    metrics = trainer_fold.evaluate() # <<<< MAKE SURE THIS LINE IS PRESENT AND NOT COMMENTED OUT\n",
    "\n",
    "    all_fold_metrics.append(metrics) # <<<< MAKE SURE THIS LINE IS PRESENT AND NOT COMMENTED OUT\n",
    "    print(f\"Metrics for Fold {fold_num + 1}: {metrics}\") # This will print the metrics dictionary for the fold\n",
    "\n",
    "    print(\"\\n===== Cross-Validation Results Summary =====\")\n",
    "\n",
    "if all_fold_metrics: # This is the list you'd populate in your CV loop\n",
    "    # --- Define the metric keys we expect from trainer.evaluate() ---\n",
    "    # These are based on your compute_classification_metrics function,\n",
    "    # with \"eval_\" prepended by the Trainer.\n",
    "    f1_weighted_key = 'eval_f1_weighted'\n",
    "    f1_macro_key = 'eval_f1_macro' # Assuming you might have added this based on my earlier suggestions\n",
    "    accuracy_key = 'eval_accuracy'\n",
    "    precision_weighted_key = 'eval_precision_weighted'\n",
    "    recall_weighted_key = 'eval_recall_weighted'\n",
    "\n",
    "    # Check if the primary key for F1 exists, to avoid errors if a fold failed or metrics changed\n",
    "    if not all_fold_metrics[0] or f1_weighted_key not in all_fold_metrics[0]:\n",
    "        print(f\"Warning: Key '{f1_weighted_key}' not found in the first fold's metrics.\")\n",
    "        print(f\"Available keys in first fold: {all_fold_metrics[0].keys() if all_fold_metrics[0] else 'N/A'}\")\n",
    "        # Attempt to find a similar key if the exact one is missing (e.g. if you renamed it in compute_metrics)\n",
    "        potential_f1_keys = [k for k in (all_fold_metrics[0] or {}).keys() if 'f1_weighted' in k]\n",
    "        if potential_f1_keys:\n",
    "            f1_weighted_key = potential_f1_keys[0]\n",
    "            print(f\"Using alternative key for F1 weighted: '{f1_weighted_key}'\")\n",
    "        else:\n",
    "            f1_weighted_key = None # Cannot calculate average F1 weighted\n",
    "\n",
    "    if f1_weighted_key:\n",
    "        avg_f1_weighted = np.mean([m.get(f1_weighted_key, 0) for m in all_fold_metrics])\n",
    "        print(f\"Average {f1_weighted_key} across {N_SPLITS} folds: {avg_f1_weighted:.4f}\")\n",
    "\n",
    "    # Calculate and print average for other metrics if they exist\n",
    "    if f1_macro_key in all_fold_metrics[0]: # Check if macro F1 was calculated\n",
    "        avg_f1_macro = np.mean([m.get(f1_macro_key, 0) for m in all_fold_metrics])\n",
    "        print(f\"Average {f1_macro_key} across {N_SPLITS} folds: {avg_f1_macro:.4f}\")\n",
    "\n",
    "    avg_accuracy = np.mean([m.get(accuracy_key, 0) for m in all_fold_metrics])\n",
    "    print(f\"Average {accuracy_key} across {N_SPLITS} folds: {avg_accuracy:.4f}\")\n",
    "\n",
    "    avg_precision_weighted = np.mean([m.get(precision_weighted_key, 0) for m in all_fold_metrics])\n",
    "    print(f\"Average {precision_weighted_key} across {N_SPLITS} folds: {avg_precision_weighted:.4f}\")\n",
    "\n",
    "    avg_recall_weighted = np.mean([m.get(recall_weighted_key, 0) for m in all_fold_metrics])\n",
    "    print(f\"Average {recall_weighted_key} across {N_SPLITS} folds: {avg_recall_weighted:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Individual Fold Metrics ---\")\n",
    "    for i, metrics_dict in enumerate(all_fold_metrics):\n",
    "        print(f\"Fold {i + 1}/{N_SPLITS}:\")\n",
    "        for metric_name, metric_value in metrics_dict.items():\n",
    "            # Only print metrics that are simple numerical values for cleaner output\n",
    "            if isinstance(metric_value, (int, float)):\n",
    "                print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "            else:\n",
    "                print(f\"  {metric_name}: {metric_value}\") # e.g. runtime, samples_per_second\n",
    "else:\n",
    "    print(\"No fold metrics were collected. Ensure your cross-validation loop ran and appended results to 'all_fold_metrics'.\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80e2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable being used by this session: /Users/Long/Documents/NLP_SentimentAnalysis_Drug/nlp_env/bin/python\n",
      "------------------------------\n",
      "Running: !pip show accelerate (or equivalent)\n",
      "Output of 'pip show accelerate':\n",
      "\u001b[33mWARNING: Package(s) not found: accelerate\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "------------------------------\n",
      "'accelerate' library could NOT be imported. It's likely not installed in this environment or the environment path is incorrect.\n",
      "------------------------------\n",
      "Attempting to instantiate TrainingArguments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Long/Documents/NLP_SentimentAnalysis_Drug/nlp_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/Long/Documents/NLP_SentimentAnalysis_Drug/nlp_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version for this attempt: 4.51.3\n",
      "STILL FAILED with TypeError: __init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "If you see this with transformers 4.51.3, something is very unusual about your environment or TrainingArguments import.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418e468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
